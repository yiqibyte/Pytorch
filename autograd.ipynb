{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  PyTorch è‡ªåŠ¨æ±‚å¯¼æœºåˆ¶è¦ç‚¹ç¬”è®°\n",
    "\n",
    "## 1. åˆ›å»ºå¶å­èŠ‚ç‚¹ï¼ˆLeaf Nodeï¼‰å¼ é‡\n",
    "- ä½¿ç”¨ `requires_grad=True` æŒ‡å®šæ˜¯å¦è®°å½•å¯¹è¯¥å¼ é‡çš„æ“ä½œï¼›\n",
    "- é»˜è®¤ä¸º `False`ï¼›\n",
    "- å¦‚æœåç»­æ¶‰åŠä¾èµ–æ“ä½œï¼ŒPyTorch ä¼šè‡ªåŠ¨å°†å…¶è®¾ä¸º `True`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 2. æ§åˆ¶æ˜¯å¦è®°å½•æ¢¯åº¦\n",
    "- ä½¿ç”¨ `.requires_grad_()` æ–¹æ³•æ§åˆ¶ï¼›\n",
    "- ä½¿ç”¨ `.detach()` æˆ– `with torch.no_grad()` å¯åœæ­¢æ¢¯åº¦è®°å½•ï¼ˆä¸è¿½è¸ªå†å²ï¼‰ï¼›\n",
    "- å¸¸ç”¨äºæ¨¡å‹è¯„ä¼°å’Œæµ‹è¯•é˜¶æ®µã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 3. è¿ç®—ç”Ÿæˆçš„å¼ é‡ï¼ˆéå¶å­èŠ‚ç‚¹ï¼‰\n",
    "- ä¼šè‡ªåŠ¨å¸¦æœ‰ `grad_fn` å±æ€§ï¼Œè¡¨ç¤ºå®ƒçš„æ¢¯åº¦å‡½æ•°ï¼›\n",
    "- å¶å­èŠ‚ç‚¹çš„ `grad_fn` ä¸º `None`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 4. è°ƒç”¨ `.backward()` æ–¹æ³•\n",
    "- ä¼šä»è¾“å‡ºå¼ é‡å¼€å§‹åå‘ä¼ æ’­ï¼Œè‡ªåŠ¨è®¡ç®—æ¢¯åº¦ï¼›\n",
    "- æ¢¯åº¦å°†ç´¯åŠ åˆ° `.grad` å±æ€§ä¸­ï¼›\n",
    "- è®¡ç®—å®Œæˆåï¼Œéå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦ä¼šè¢«é‡Šæ”¾ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 5. `.backward()` å‚æ•°è¦æ±‚\n",
    "- è¾“å…¥å‚æ•°åº”å’Œè°ƒç”¨ `.backward()` çš„å¼ é‡ç»´åº¦ç›¸åŒï¼›\n",
    "- æ”¯æŒ broadcastï¼›\n",
    "- å¦‚æœå¼ é‡æ˜¯æ ‡é‡ï¼ˆ0 ç»´ï¼‰ï¼Œåˆ™å‚æ•°å¯çœç•¥ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 6. å¤šæ¬¡åå‘ä¼ æ’­éœ€æ³¨æ„\n",
    "- ä¸­é—´ç¼“å­˜é»˜è®¤ä¼šè¢«é‡Šæ”¾ï¼›\n",
    "- å¦‚æœéœ€è¦å¤šæ¬¡åå‘ä¼ æ’­ï¼Œè¯·è®¾ç½® `retain_graph=True`ï¼›\n",
    "- æ¯æ¬¡åå‘ä¼ æ’­æ—¶ï¼Œæ¢¯åº¦æ˜¯ç´¯åŠ çš„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 7. `.backward()` åæ¢¯åº¦é»˜è®¤æ¸…ç©º\n",
    "- éå¶å­èŠ‚ç‚¹çš„ `.grad` ä¼šè¢«æ¸…ç©ºï¼›\n",
    "- å¦‚éœ€ä¿ç•™ï¼Œè¯·æ‰‹åŠ¨ä¿å­˜æˆ– `retain_graph=True`ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## 8. `no_grad` è¯­å¢ƒæ§åˆ¶\n",
    "- ä½¿ç”¨ `torch.no_grad()` æˆ–è®¾ç½® `y.requires_grad=False` å¯æ˜¾å¼å…³é—­è¿½è¸ªï¼›\n",
    "- åœ¨æ¨ç†é˜¶æ®µå¹¿æ³›ä½¿ç”¨ï¼Œå¯èŠ‚çœå†…å­˜ä¸è®¡ç®—ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” è®¡ç®—å›¾ç»“æ„è¯´æ˜\n",
    "- PyTorch æ„å»ºçš„è®¡ç®—å›¾æ˜¯**åŠ¨æ€å›¾**ï¼Œæ¯æ¬¡å‰å‘ä¼ æ’­æ—¶é‡å»ºï¼›\n",
    "- ä¸ TensorFlow çš„é™æ€å›¾ä¸åŒï¼›\n",
    "- æ•´ä¸ªå›¾ç»“æ„æ˜¯**æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "print(x.requires_grad)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "y = x.detach()\n",
    "print(y.requires_grad)  # False\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f64903430d0>\n"
     ]
    }
   ],
   "source": [
    "z = x * 3 + 1\n",
    "print(z.grad_fn)  # æœ‰ grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "out = z.sum()\n",
    "out.backward()\n",
    "print(x.grad)  # x çš„æ¢¯åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = x ** 2\n",
    "grad_output = torch.ones_like(y)\n",
    "y.backward(grad_output)  # å¿…é¡»ä¼ å‚ï¼Œå› ä¸º y ä¸æ˜¯æ ‡é‡\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x * 2\n",
    "y.sum().backward(retain_graph=True)\n",
    "print(x.grad) # tensor([2., 2., 2.])\n",
    "y.sum().backward()  # ç¬¬äºŒæ¬¡ä¼ æ’­éœ€ä¿ç•™å›¾\n",
    "print(x.grad) # tensor([4., 4., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_614/2864770302.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(b.grad)  # None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = a * 3  # b æ˜¯éå¶å­èŠ‚ç‚¹\n",
    "loss = b.sum()\n",
    "loss.backward()\n",
    "print(b.grad)  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = x * 3\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([36.])\n",
      "y.grad: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# åˆ›å»ºå¶å­èŠ‚ç‚¹\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# ç”Ÿæˆéå¶å­èŠ‚ç‚¹\n",
    "y = x * 3\n",
    "y.retain_grad()  # â­ ä¿ç•™ä¸­é—´å˜é‡çš„æ¢¯åº¦\n",
    "\n",
    "# è®¡ç®—ç»“æœ\n",
    "z = y ** 2\n",
    "z.backward()\n",
    "\n",
    "# å¶å­èŠ‚ç‚¹çš„æ¢¯åº¦\n",
    "print(\"x.grad:\", x.grad)  # âœ… æœ‰æ¢¯åº¦\n",
    "\n",
    "# éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦\n",
    "print(\"y.grad:\", y.grad)  # âœ… æœ‰æ¢¯åº¦ï¼Œå› ä¸ºè°ƒç”¨äº† retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tensor([12.], grad_fn=<MulBackward0>)\n",
      "dÂ²y/dxÂ²: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# 1. åˆ›å»ºå¼ é‡ï¼Œè®¾ç½® requires_grad=True æ‰èƒ½å‚ä¸è‡ªåŠ¨æ±‚å¯¼\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 2. å®šä¹‰å‡½æ•° y = x^3\n",
    "y = x ** 3\n",
    "\n",
    "# 3. æ±‚ä¸€é˜¶å¯¼æ•° dy/dx = 3x^2\n",
    "dy_dx = torch.autograd.grad(outputs=y, inputs=x, create_graph=True)[0]\n",
    "print(\"dy/dx:\", dy_dx)  # è¾“å‡º: tensor([12.])\n",
    "\n",
    "# 4. å¯¹ä¸€é˜¶å¯¼å†æ±‚å¯¼ï¼ˆäºŒé˜¶å¯¼æ•°ï¼‰dÂ²y/dxÂ² = 6x\n",
    "d2y_dx2 = torch.autograd.grad(outputs=dy_dx, inputs=x)[0]\n",
    "print(\"dÂ²y/dxÂ²:\", d2y_dx2)  # è¾“å‡º: tensor([6.])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
