{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 PyTorch 自动求导机制要点笔记\n",
    "\n",
    "## 1. 创建叶子节点（Leaf Node）张量\n",
    "- 使用 `requires_grad=True` 指定是否记录对该张量的操作；\n",
    "- 默认为 `False`；\n",
    "- 如果后续涉及依赖操作，PyTorch 会自动将其设为 `True`。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 控制是否记录梯度\n",
    "- 使用 `.requires_grad_()` 方法控制；\n",
    "- 使用 `.detach()` 或 `with torch.no_grad()` 可停止梯度记录（不追踪历史）；\n",
    "- 常用于模型评估和测试阶段。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 运算生成的张量（非叶子节点）\n",
    "- 会自动带有 `grad_fn` 属性，表示它的梯度函数；\n",
    "- 叶子节点的 `grad_fn` 为 `None`。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 调用 `.backward()` 方法\n",
    "- 会从输出张量开始反向传播，自动计算梯度；\n",
    "- 梯度将累加到 `.grad` 属性中；\n",
    "- 计算完成后，非叶子节点的梯度会被释放。\n",
    "\n",
    "---\n",
    "\n",
    "## 5. `.backward()` 参数要求\n",
    "- 输入参数应和调用 `.backward()` 的张量维度相同；\n",
    "- 支持 broadcast；\n",
    "- 如果张量是标量（0 维），则参数可省略。\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 多次反向传播需注意\n",
    "- 中间缓存默认会被释放；\n",
    "- 如果需要多次反向传播，请设置 `retain_graph=True`；\n",
    "- 每次反向传播时，梯度是累加的。\n",
    "\n",
    "---\n",
    "\n",
    "## 7. `.backward()` 后梯度默认清空\n",
    "- 非叶子节点的 `.grad` 会被清空；\n",
    "- 如需保留，请手动保存或 `retain_graph=True`。\n",
    "\n",
    "---\n",
    "\n",
    "## 8. `no_grad` 语境控制\n",
    "- 使用 `torch.no_grad()` 或设置 `y.requires_grad=False` 可显式关闭追踪；\n",
    "- 在推理阶段广泛使用，可节省内存与计算。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 计算图结构说明\n",
    "- PyTorch 构建的计算图是**动态图**，每次前向传播时重建；\n",
    "- 与 TensorFlow 的静态图不同；\n",
    "- 整个图结构是**有向无环图（DAG）**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "print(x.requires_grad)  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "y = x.detach()\n",
    "print(y.requires_grad)  # False\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = x * 2\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f64903430d0>\n"
     ]
    }
   ],
   "source": [
    "z = x * 3 + 1\n",
    "print(z.grad_fn)  # 有 grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3.])\n"
     ]
    }
   ],
   "source": [
    "out = z.sum()\n",
    "out.backward()\n",
    "print(x.grad)  # x 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = x ** 2\n",
    "grad_output = torch.ones_like(y)\n",
    "y.backward(grad_output)  # 必须传参，因为 y 不是标量\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2.])\n",
      "tensor([4., 4., 4.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = x * 2\n",
    "y.sum().backward(retain_graph=True)\n",
    "print(x.grad) # tensor([2., 2., 2.])\n",
    "y.sum().backward()  # 第二次传播需保留图\n",
    "print(x.grad) # tensor([4., 4., 4.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_614/2864770302.py:5: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  print(b.grad)  # None\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([2.0], requires_grad=True)\n",
    "b = a * 3  # b 是非叶子节点\n",
    "loss = b.sum()\n",
    "loss.backward()\n",
    "print(b.grad)  # None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    z = x * 3\n",
    "print(z.requires_grad)  # False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([36.])\n",
      "y.grad: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# 创建叶子节点\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 生成非叶子节点\n",
    "y = x * 3\n",
    "y.retain_grad()  # ⭐ 保留中间变量的梯度\n",
    "\n",
    "# 计算结果\n",
    "z = y ** 2\n",
    "z.backward()\n",
    "\n",
    "# 叶子节点的梯度\n",
    "print(\"x.grad:\", x.grad)  # ✅ 有梯度\n",
    "\n",
    "# 非叶子节点的梯度\n",
    "print(\"y.grad:\", y.grad)  # ✅ 有梯度，因为调用了 retain_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx: tensor([12.], grad_fn=<MulBackward0>)\n",
      "d²y/dx²: tensor([12.])\n"
     ]
    }
   ],
   "source": [
    "# 1. 创建张量，设置 requires_grad=True 才能参与自动求导\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# 2. 定义函数 y = x^3\n",
    "y = x ** 3\n",
    "\n",
    "# 3. 求一阶导数 dy/dx = 3x^2\n",
    "dy_dx = torch.autograd.grad(outputs=y, inputs=x, create_graph=True)[0]\n",
    "print(\"dy/dx:\", dy_dx)  # 输出: tensor([12.])\n",
    "\n",
    "# 4. 对一阶导再求导（二阶导数）d²y/dx² = 6x\n",
    "d2y_dx2 = torch.autograd.grad(outputs=dy_dx, inputs=x)[0]\n",
    "print(\"d²y/dx²:\", d2y_dx2)  # 输出: tensor([6.])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
